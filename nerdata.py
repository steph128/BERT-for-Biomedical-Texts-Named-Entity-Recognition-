# -*- coding: utf-8 -*-
"""NERData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YIzyVHEiR_Xm-psENEbOfXVygZ9C77Bx
"""

import numpy as np
from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig

from transformers import BertForTokenClassification, AdamW
import torch
from torch.utils.data import TensorDataset, DataLoader, random_split
from torch.utils.data import RandomSampler, SequentialSampler

from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

from transformers import get_linear_schedule_with_warmup

from tqdm import tqdm, trange
from seqeval.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report


def get_sentences(df):
    tokens = df.tokens.values
    labels = df.labels.values

    train_sentences = []
    train_labels = []
    temp_sentence = []
    temp_label = []

    for token, label in zip(tokens, labels):
        if token != ".":
            temp_sentence.append(token)
            temp_label.append(label)
        else:
            temp_sentence.append(".")
            temp_label.append("O")
            train_sentences.append(temp_sentence)
            train_labels.append(temp_label)
            temp_sentence = []
            temp_label = []
    return (train_sentences, train_labels)


def get_unique_labels(df):
    unique_labels = list(df.labels.unique())
    unique_labels.append("PAD")
    id_to_label = {i: label for i, label in enumerate(unique_labels)}
    label_to_id = {label: i for i, label in id_to_label.items()}
    return unique_labels, id_to_label, label_to_id


def tokenize_and_preserve_labels(tokenizer, sentence, text_labels):
    tokenized_sentence = []
    labels = []
    for word, label in zip(sentence, text_labels):
        tokenized_word = tokenizer.tokenize(word)
        n_subwords = len(tokenized_word)

        tokenized_sentence.extend(tokenized_word)
        labels.extend([label] * n_subwords)

    return tokenized_sentence, labels


def padding(tokenizer, train_tokenized_texts, train_labels, label_to_id, maxlen=125, dtype="long", value=0.0,
            truncating="post", padding="post"):
    train_input = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in train_tokenized_texts],
                                maxlen=maxlen, dtype=dtype, value=value,
                                truncating=truncating, padding=padding)

    train_tags = pad_sequences([[label_to_id.get(label) for label in label_sent] for label_sent in train_labels],
                               maxlen=maxlen, value=label_to_id["PAD"], padding=padding,
                               dtype=dtype, truncating=truncating)

    return train_input, train_tags


def preprocess(train_input, train_tags, batch_size):
    attention_masks = [[float(i != 0.0) for i in train_sentence] for train_sentence in train_input]
    # ignore padded region
    tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(train_input, train_tags, random_state=42, test_size=0.1)
    tr_masks, val_masks, _, _ = train_test_split(attention_masks, train_input,
                                                 random_state=42, test_size=0.1)

    tr_inputs = torch.tensor(tr_inputs)
    val_inputs = torch.tensor(val_inputs)
    tr_tags = torch.tensor(tr_tags)
    val_tags = torch.tensor(val_tags)
    tr_masks = torch.tensor(tr_masks)
    val_masks = torch.tensor(val_masks)

    train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)
    train_sampler = RandomSampler(train_data)
    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

    valid_data = TensorDataset(val_inputs, val_masks, val_tags)
    valid_sampler = SequentialSampler(valid_data)
    valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)

    return train_dataloader, valid_dataloader


# optimizer and weight decay for regularization 


def create_optimizer(model, lr=3e-5, eps=1e-8):
    param_optimizer = list(model.named_parameters())
    no_decay = ["bias", "gamma", "beta"]
    optimizer_grouped_parameters = [
        {"params": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
         "weight_decay_rate": 0.01},
        {"params": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
         "weight_decay_rate": 0.0}
    ]
    optimizer = AdamW(
        optimizer_grouped_parameters,
        lr=3e-5,
        eps=1e-8
    )
    return optimizer


def create_scheduler(optimizer, train_dataloader, epochs=3):
    total_steps = len(train_dataloader) * epochs
    # Create the learning rate scheduler.
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=0,
        num_training_steps=total_steps
    )
    return scheduler


def training(model, optimizer, scheduler, train_dataloader, valid_dataloader, unique_labels, device, epochs=3,
             max_grad_norm=1.0):
    loss_values, validation_loss_values = [], []  # store for each epoch
    for _ in trange(epochs, desc="Epoch"):  # prefix message desc
        model.train()
        total_loss = 0

        for step, batch in enumerate(train_dataloader):
            batch = tuple(t.to(device) for t in batch)
            b_input_ids, b_input_mask, b_labels = batch

            b_input_ids = b_input_ids.type(torch.LongTensor).to(device)
            b_input_mask = b_input_mask.type(torch.LongTensor).to(device)
            b_labels = b_labels.type(torch.LongTensor).to(device)

            model.zero_grad()  # clear prev. accumulated gradients

            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,
                            labels=b_labels)
            loss = outputs[0]
            loss.backward()
            total_loss += loss.item()
            # clip gradients to prevent exploding gradients
            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
            optimizer.step()
            scheduler.step()  # updated learning rate

        # calculate average loss over the batches
        avg_train_loss = total_loss / len(train_dataloader)
        print("Average train loss:{}".format(avg_train_loss))

        loss_values.append(avg_train_loss)

        model.eval()
        eval_loss, eval_accuracy = 0, 0
        nb_eval_steps, nb_eval_examples = 0, 0

        predictions, true_labels = [], []
        for batch in valid_dataloader:
            batch = tuple(t.to(device) for t in batch)
            b_input_ids, b_input_mask, b_labels = batch

            b_input_ids = b_input_ids.type(torch.LongTensor).to(device)
            b_input_mask = b_input_mask.type(torch.LongTensor).to(device)
            b_labels = b_labels.type(torch.LongTensor).to(device)

            with torch.no_grad():
                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,
                                labels=b_labels)
                logits = outputs[1].detach().cpu().numpy()  # (7, 100, 30) # 100 is max len, 30 is no. labels
                label_ids = b_labels.to("cpu").numpy()

                eval_loss += outputs[0].mean().item()  # add loss for this batch
                predictions.extend([list(p) for p in np.argmax(logits,
                                                               axis=2)])  # 8639 for 100 each sentence (axis 2 is 30- labels prob)
                true_labels.extend(label_ids)

        eval_loss = eval_loss / len(valid_dataloader)  # average of all the batches
        print("Validation loss:{}.format(eval_loss")
        validation_loss_values.append(eval_loss)

        # get actual labels (label_to_id) for predictions
        temp = []
        final_predictions = []
        for sent_no, pred in enumerate(predictions):
            for i, p in enumerate(pred):
                if true_labels[sent_no][i] != 29:
                    temp.append(unique_labels[p])
            final_predictions.append(temp)
            temp = []

        # get actual labels for true labels
        real_labels = []
        temp2 = []
        for sent in true_labels:
            for lab in sent:
                if lab != 29:
                    temp2.append(unique_labels[lab])
            real_labels.append(temp2)
            temp2 = []

        print("Validation Accuracy: {}".format(accuracy_score(real_labels, final_predictions)))
        print("Validation F1-Score: {}".format(f1_score(real_labels, final_predictions)))
        print("Precision: {}".format(precision_score(real_labels, final_predictions)))
        print("Recall: {}".format(recall_score(real_labels, final_predictions)))
